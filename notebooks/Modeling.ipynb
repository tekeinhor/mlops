{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ce29d3fb",
   "metadata": {},
   "source": [
    "# Experimentation - Exploration"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f575ee8",
   "metadata": {},
   "source": [
    "In our previous notebook \"EDA\", we have noticed that our data was imbalanced. Therefore, we were confronted to an anomaly dectection problem. \n",
    "Our objective will be to correctly classify the minority class of `Fake`events.\n",
    "\n",
    "What we are going to do:\n",
    "- Feature selection\n",
    "- This imbalance can be reduced by under-sampling the majority class `Not fake`, by making it close to that of the `Fake` class.\n",
    "- Model Training\n",
    "- Model Evaluation\n",
    "\n",
    "Time for experimentation\n",
    "There are no \"Null\" values, so we don't have to work on ways to replace values. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c093574a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01bce3c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"../data/fake_users.csv\")\n",
    "\n",
    "# Unnamed and UserId are dataset artifact, not something useful for analysis\n",
    "df.drop(\"Unnamed: 0\", axis=1, inplace=True)\n",
    "df.drop(\"UserId\", axis=1, inplace=True)\n",
    "\n",
    "# let's make a copy of our orignal dataset\n",
    "df_feat = df.copy()\n",
    "\n",
    "# A quick reminder of how the data looks like\n",
    "df_feat.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a59d031",
   "metadata": {},
   "source": [
    "# Featurize\n",
    "\n",
    "Our `Event` and `Category` columns hold nominal categorial data where there are no inherent order (in opposition to ordinal categorial data). These \"categories\" must be transformed into numbers first, before you can apply the learning algorithm on them.\n",
    "\n",
    "To achieve that there are different encoding techniques.\n",
    "- Label Encoding: each label is converted into an integer value based on conversion dictionnary \n",
    "- One Hot Encoding: each category is mapped with a binary variable containing either 0 or 1. Here, 0 represents the absence, and 1 represents the presence of that category.\n",
    "- Hash Encoding: each category is encoded using a hash function. It is a good solution when the cardinality of the category is too high.  \n",
    "\n",
    "We will user one hot encoding, because the cardinality of our categories are rather low."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2bd70908",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_feat = pd.get_dummies(df_feat, columns=['Event'], prefix = ['Event'])\n",
    "df_feat = pd.get_dummies(df_feat, columns=['Category'], prefix = ['Category'])\n",
    "df_feat.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c0d1f1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "original_df_feat = df_feat.copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dcbb7f46",
   "metadata": {},
   "source": [
    "Now let's separate our class from the rest of the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "978de06f",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train = df_feat.pop(\"Fake\")\n",
    "X_train = df_feat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f695dc6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'Dimension of training, X: {X_train.shape}, y: {y_train.shape}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b53db5b9",
   "metadata": {},
   "source": [
    "# Training\n",
    "\n",
    "Now, let's proceed to the traning of models. Because our data is imbalanced we have to decide which metrics to use to evaluate the model.\n",
    "We will use:\n",
    "- log"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ca0e95e",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''Importing the auxiliar and preprocessing librarys'''\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
    "from sklearn.model_selection import GridSearchCV, KFold, StratifiedKFold, cross_val_score, cross_val_predict\n",
    "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "'''Initialize all the regression models object we are interested in.'''\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.dummy import DummyClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.svm import SVC\n",
    "'''Plotly visualization .'''\n",
    "import plotly.offline as py\n",
    "import plotly.graph_objs as go\n",
    "import plotly.tools as tls\n",
    "py.init_notebook_mode(connected=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a455b75",
   "metadata": {},
   "outputs": [],
   "source": [
    "def GetBasedModel():\n",
    "    basedModels = []\n",
    "    \n",
    "    basedModels.append(('DUM', DummyClassifier()))\n",
    "    basedModels.append(('LR', Pipeline([('Scaler', StandardScaler()), \n",
    "                                   ('LR', LogisticRegression())])))\n",
    "    basedModels.append(('KNN' , KNeighborsClassifier()))\n",
    "    \n",
    "    \n",
    "    return basedModels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43f71771",
   "metadata": {},
   "outputs": [],
   "source": [
    "def computeMetrics(conf_matrix):\n",
    "    tn, fp, fn, tp = conf_matrix.reshape(-1)\n",
    "    #print(f\"TP:{tp}, FP:{fp}, FN: {fn}, TN:{tn}\")\n",
    "    \n",
    "    precision = 0 if tp+fp==0 else tp/(tp+fp)\n",
    "    recall = 0 if tp+fn== 0 else tp/(tp+fn)\n",
    "    f1 = 0 if precision+recall == 0 else 2*(precision*recall)/(precision+recall)\n",
    "    \n",
    "    print(f\"precision: {precision:.2f}\\nrecall: {recall:.2f}\\nf1-score: {f1:.2f}\")\n",
    "    print('---' * 35)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a0d2167",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import recall_score, precision_score, f1_score, accuracy_score\n",
    "\n",
    "'''Test options and evaluation metric'''\n",
    "def BasedLine(X_train, y_train, models, scoring = \"accuray\", num_folds = 10):\n",
    "\n",
    "    results, names = [], []\n",
    "\n",
    "    for name, model in models:\n",
    "        kfold = StratifiedKFold(n_splits = num_folds)\n",
    "        \n",
    "        y_pred = cross_val_predict(model, X_train, y_train, cv=kfold)\n",
    "        conf_mat = confusion_matrix(y_train, y_pred)\n",
    "        \n",
    "        cv_results = cross_val_score(model, X_train, y_train, cv = kfold, scoring = scoring, n_jobs = -1)\n",
    "\n",
    "\n",
    "        names.append(name)\n",
    "        print(f\"{name}\\n{pd.DataFrame(conf_mat)}\\n\")\n",
    "        computeMetrics(conf_mat)\n",
    "\n",
    "    \n",
    "    return names, results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58550fad",
   "metadata": {},
   "outputs": [],
   "source": [
    "models = GetBasedModel()\n",
    "names,results = BasedLine(X_train, y_train,models, \"f1\", 10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a1bb2c6",
   "metadata": {},
   "source": [
    "# Sampling\n",
    "\n",
    "There are several sampling techniques:\n",
    "- Undersampling: select a subset of examples from the majority class.\n",
    "- Oversampling: duplicate examples in the minority class or synthesize new examples from the examples in the minority class.\n",
    "- Combinations of Techniques: applying both undersampling and oversampling techniques together.\n",
    "\n",
    "Each technique has several implementation methods. In our case we will use Random Undersampling. It consists in selecting randomly a subest of the majority class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2801c852",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_feat = original_df_feat.sample(frac=1)\n",
    "\n",
    "nb_fake = df_feat['Fake'].value_counts()[1]\n",
    "print(nb_fake)\n",
    "\n",
    "# amount of fraud classes 10359 rows.\n",
    "fake_df = df_feat.loc[df_feat['Fake'] == 1]\n",
    "not_fake_df = df_feat.loc[df_feat['Fake'] == 0][:nb_fake]\n",
    "\n",
    "normal_distributed_df = pd.concat([fake_df, not_fake_df], ignore_index=True)\n",
    "\n",
    "# Shuffle dataframe rows\n",
    "new_df = normal_distributed_df.sample(frac=1, random_state=42, ignore_index=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c896d5c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.countplot(x='Fake', data=new_df, palette=\"Pastel1\")\n",
    "plt.title('Equally Distributed Classes', fontsize=14)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25b76609",
   "metadata": {},
   "outputs": [],
   "source": [
    "new_y_train = new_df.pop(\"Fake\")\n",
    "new_X_train = new_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d3a015f",
   "metadata": {},
   "outputs": [],
   "source": [
    "models = GetBasedModel()\n",
    "names,results = BasedLine(new_X_train, new_y_train,models, \"f1\", 5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9fc3a635",
   "metadata": {},
   "source": [
    "# Model tunning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38a9c846",
   "metadata": {},
   "outputs": [],
   "source": [
    "seed = 44\n",
    "def grid_search_cv(model, params, scoring=\"f1\", cv = 10):    \n",
    "    grid_search = GridSearchCV(estimator = model, param_grid = params, cv = cv, verbose = 1,\n",
    "                             scoring = scoring, n_jobs = -1)\n",
    "    grid_search.fit(new_X_train, new_y_train)\n",
    "    best_params = grid_search.best_params_\n",
    "    best_score = grid_search.best_score_\n",
    "    best_estimator = grid_search.best_estimator_\n",
    "    \n",
    "    return best_estimator, best_params, best_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e418d4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "LR_model = LogisticRegression()\n",
    "\n",
    "LR_params = [\n",
    "  {'penalty': ['l1'], 'solver': [ 'saga','liblinear'], 'C': [ 0.01,0.1, 1, 10]},\n",
    "  {'penalty': ['l2'], 'solver': ['newton-cg', 'sag', 'saga','lbfgs'], 'C': [ 0.01,0.1, 1, 10]},\n",
    " ]\n",
    "\n",
    "\n",
    "LR_best_estimator, LR_best_params, LR_best_score= grid_search_cv(LR_model, LR_params, scoring=\"f1\", cv= 5)\n",
    "print(f\"LR best params:{LR_best_params} & best_score:{LR_best_score:0.5f} / {LR_best_estimator}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23475acc",
   "metadata": {},
   "outputs": [],
   "source": [
    "knears_params = {\"n_neighbors\": [2, 3, 4, 5], 'algorithm': ['auto', 'ball_tree', 'kd_tree'], \"weights\": ['uniform', 'distance']}\n",
    "\n",
    "KNN_model = KNeighborsClassifier()\n",
    "grid_knears = GridSearchCV(KNN_model, knears_params)\n",
    "\n",
    "KNN_best_estimator, KNN_best_params, KNN_best_score= grid_search_cv(KNN_model, knears_params, scoring=\"f1\", cv= 5)\n",
    "print(f\"KNN best params:{KNN_best_params} & best_score:{KNN_best_score:0.3f} / {KNN_best_estimator}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21e6dbdb",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(grid_knears.best_estimator_, grid_knears.best_params_, grid_knears.best_score_)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aae51738",
   "metadata": {},
   "source": [
    "# Model testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b355d26",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test = pd.read_csv(\"../data/fake_users_test.csv\")\n",
    "df_test.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30243baf",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test = df_test.pop(\"Fake\")\n",
    "X_test = df_test\n",
    "\n",
    "X_test_f= df_test.iloc[:,1:]\n",
    "X_test_f = pd.get_dummies(X_test_f, columns=['Event', 'Category'], prefix = ['Event', 'Category'])\n",
    "X_test_f.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e1e37da",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import recall_score, precision_score, f1_score, accuracy_score\n",
    "\n",
    "knn = KNeighborsClassifier(n_neighbors=5)\n",
    "knn.fit(new_X_train, new_y_train)\n",
    "\n",
    "\n",
    "y_pred = knn.predict(X_test_f)\n",
    "y_proba = knn.predict_proba(X_test_f)\n",
    "\n",
    "\n",
    "# Overfitting Case\n",
    "print('---' * 35)\n",
    "print(f'Recall: {recall_score(y_test, y_pred):.2f}')\n",
    "print(f'Precision: {precision_score(y_test, y_pred):.2f}')\n",
    "print(f'F1 Score: {f1_score(y_test, y_test):.2f}')\n",
    "print(f'Accuracy Score: {accuracy_score(y_test, y_pred):.2f}')\n",
    "print('---' * 35)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64b71ac4",
   "metadata": {},
   "outputs": [],
   "source": [
    "sub = pd.DataFrame(data=X_test)\n",
    "sub['Fake_pred'] = y_pred\n",
    "sub['is_fake_probability'] =  y_proba[:,1]\n",
    "sub.tail()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86e91921",
   "metadata": {},
   "source": [
    "# Conclusion\n",
    "\n",
    "A limitation of random undersampling is that examples are removed without any concern for how useful or important they might be in determining the decision boundary between the classes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e812cc4c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:lbc] *",
   "language": "python",
   "name": "conda-env-lbc-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
